{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8154bc87-fa0b-43aa-ac0d-36cb2e4a396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries for statistical, plot analysis and regression \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e9d181-3fdf-496c-8e11-f0d164118930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>refdate</th>\n",
       "      <th>monthly_date</th>\n",
       "      <th>TimeIndicator</th>\n",
       "      <th>ROR</th>\n",
       "      <th>AUM_Filled</th>\n",
       "      <th>T_Bill</th>\n",
       "      <th>downside</th>\n",
       "      <th>upside</th>\n",
       "      <th>currencycode</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>month_of_quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>Net cash flow</th>\n",
       "      <th>style_return</th>\n",
       "      <th>st_adj_rateofreturn</th>\n",
       "      <th>DeltaMgr</th>\n",
       "      <th>DeltaOwn</th>\n",
       "      <th>TotalDelta</th>\n",
       "      <th>TotalDelta_Winso_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1993-01-31</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>192</td>\n",
       "      <td>-0.035175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-01-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1993-02-28</td>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>193</td>\n",
       "      <td>-0.050656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-02-28 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1993-03-31</td>\n",
       "      <td>1993-03-01</td>\n",
       "      <td>194</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-03-31 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1993-04-30</td>\n",
       "      <td>1993-04-01</td>\n",
       "      <td>195</td>\n",
       "      <td>0.087431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085039</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-04-30 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1993-05-31</td>\n",
       "      <td>1993-05-01</td>\n",
       "      <td>196</td>\n",
       "      <td>0.079997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077530</td>\n",
       "      <td>USD</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-05-31 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ref    refdate monthly_date  TimeIndicator       ROR  AUM_Filled    T_Bill  \\\n",
       "0    5 1993-01-31   1993-01-01            192 -0.035175         NaN  0.002500   \n",
       "1    5 1993-02-28   1993-02-01            193 -0.050656         NaN  0.002442   \n",
       "2    5 1993-03-31   1993-03-01            194  0.022073         NaN  0.002458   \n",
       "3    5 1993-04-30   1993-04-01            195  0.087431         NaN  0.002392   \n",
       "4    5 1993-05-31   1993-05-01            196  0.079997         NaN  0.002467   \n",
       "\n",
       "   downside    upside currencycode  ...                 date  \\\n",
       "0  0.001419  0.000000          USD  ...  1993-01-31 00:00:00   \n",
       "1  0.002819  0.000000          USD  ...  1993-02-28 00:00:00   \n",
       "2  0.000000  0.019615          USD  ...  1993-03-31 00:00:00   \n",
       "3  0.000000  0.085039          USD  ...  1993-04-30 00:00:00   \n",
       "4  0.000000  0.077530          USD  ...  1993-05-31 00:00:00   \n",
       "\n",
       "   month_of_quarter  month  Net cash flow  style_return  st_adj_rateofreturn  \\\n",
       "0                 1      1            0.0           NaN                  NaN   \n",
       "1                 2      2            0.0           NaN                  NaN   \n",
       "2                 3      3            0.0           NaN                  NaN   \n",
       "3                 1      4            0.0           NaN                  NaN   \n",
       "4                 2      5            0.0           NaN                  NaN   \n",
       "\n",
       "   DeltaMgr  DeltaOwn  TotalDelta  TotalDelta_Winso_01  \n",
       "0       NaN       NaN         NaN                  NaN  \n",
       "1       NaN       NaN         NaN                  NaN  \n",
       "2       NaN       NaN         NaN                  NaN  \n",
       "3       NaN       NaN         NaN                  NaN  \n",
       "4       NaN       NaN         NaN                  NaN  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Lipper TASS hedge fund data\n",
    "TASS = pd.read_excel(\"C:/Master Thesis final/TASS_data.xlsx\")\n",
    "TASS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491dbd3-c5ce-495b-bf38-1e49228cfee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444478f9-4a6b-4f21-a140-5cb47fd258d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30af176f-5c92-4e55-93f4-289f219a9842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ref</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>18</th>\n",
       "      <th>23</th>\n",
       "      <th>26</th>\n",
       "      <th>31</th>\n",
       "      <th>35</th>\n",
       "      <th>...</th>\n",
       "      <th>105709</th>\n",
       "      <th>105710</th>\n",
       "      <th>105756</th>\n",
       "      <th>105785</th>\n",
       "      <th>105832</th>\n",
       "      <th>105870</th>\n",
       "      <th>106079</th>\n",
       "      <th>106080</th>\n",
       "      <th>106103</th>\n",
       "      <th>106134</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-01-31</th>\n",
       "      <td>-0.035175</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.045809</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>-0.1790</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.00637</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>-0.005620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-28</th>\n",
       "      <td>-0.050656</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.050593</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.022409</td>\n",
       "      <td>-0.1790</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.00637</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>-0.005620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-03-31</th>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.052968</td>\n",
       "      <td>-0.1790</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.00637</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>-0.005620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-04-30</th>\n",
       "      <td>0.087431</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.040948</td>\n",
       "      <td>0.028120</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>-0.1790</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.02033</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>-0.005620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-05-31</th>\n",
       "      <td>0.079997</td>\n",
       "      <td>0.040761</td>\n",
       "      <td>0.058661</td>\n",
       "      <td>0.032479</td>\n",
       "      <td>0.028523</td>\n",
       "      <td>-0.1790</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>-0.005620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>-0.072000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>-0.00030</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>-0.00757</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.01239</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>-0.00286</td>\n",
       "      <td>-0.04040</td>\n",
       "      <td>-0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>-0.072000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>-0.00030</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.09222</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.01470</td>\n",
       "      <td>-0.005976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>-0.072000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>-0.00030</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.09483</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>-0.023580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>-0.072000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>-0.00030</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.05150</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03520</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>-0.072000</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>-0.049829</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>-0.00030</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.05150</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03520</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273 rows Ã— 4501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ref           5         8         9         10        11      18      23      \\\n",
       "refdate                                                                        \n",
       "1993-01-31 -0.035175  0.049116  0.185567  0.045809  0.044878 -0.1790  0.0605   \n",
       "1993-02-28 -0.050656  0.010300  0.050593  0.019571  0.022409 -0.1790  0.0605   \n",
       "1993-03-31  0.022073  0.009268  0.047404  0.040219  0.052968 -0.1790  0.0605   \n",
       "1993-04-30  0.087431  0.013774  0.040948  0.028120  0.033825 -0.1790  0.0605   \n",
       "1993-05-31  0.079997  0.040761  0.058661  0.032479  0.028523 -0.1790  0.0605   \n",
       "...              ...       ...       ...       ...       ...     ...     ...   \n",
       "2015-05-31 -0.072000  0.005688 -0.049829  0.038722  0.063176  0.0543  0.0209   \n",
       "2015-06-30 -0.072000  0.005688 -0.049829  0.038722  0.063176  0.0543  0.0209   \n",
       "2015-07-31 -0.072000  0.005688 -0.049829  0.038722  0.063176  0.0543  0.0209   \n",
       "2015-08-31 -0.072000  0.005688 -0.049829  0.038722  0.063176  0.0543  0.0209   \n",
       "2015-09-30 -0.072000  0.005688 -0.049829  0.038722  0.063176  0.0543  0.0209   \n",
       "\n",
       "ref          26      31      35      ...    105709    105710   105756  105785  \\\n",
       "refdate                              ...                                        \n",
       "1993-01-31  0.00637  0.0294  0.0260  ...  0.007131  0.006097 -0.00757  0.0601   \n",
       "1993-02-28  0.00637  0.0294  0.0120  ...  0.007131  0.006097 -0.00757  0.0601   \n",
       "1993-03-31  0.00637  0.0294 -0.0060  ...  0.007131  0.006097 -0.00757  0.0601   \n",
       "1993-04-30  0.02033  0.0294  0.0200  ...  0.007131  0.006097 -0.00757  0.0601   \n",
       "1993-05-31  0.02008  0.0294  0.0230  ...  0.007131  0.006097 -0.00757  0.0601   \n",
       "...             ...     ...     ...  ...       ...       ...      ...     ...   \n",
       "2015-05-31 -0.00030  0.0196  0.0039  ...  0.004538  0.005178 -0.00757  0.0601   \n",
       "2015-06-30 -0.00030  0.0196 -0.0132  ...  0.004538  0.005178  0.00000  0.0204   \n",
       "2015-07-31 -0.00030  0.0196 -0.0127  ...  0.004538  0.005178  0.00000  0.0024   \n",
       "2015-08-31 -0.00030  0.0196 -0.0319  ...  0.004538  0.005178  0.00000  0.0230   \n",
       "2015-09-30 -0.00030  0.0196 -0.0319  ...  0.004538  0.005178  0.00000  0.0230   \n",
       "\n",
       "ref         105832   105870  106079   106080   106103    106134  \n",
       "refdate                                                          \n",
       "1993-01-31 -0.0166  0.01640 -0.0034  0.00225  0.00084 -0.005620  \n",
       "1993-02-28 -0.0166  0.01640 -0.0034  0.00225  0.00084 -0.005620  \n",
       "1993-03-31 -0.0166  0.01640 -0.0034  0.00225  0.00084 -0.005620  \n",
       "1993-04-30 -0.0166  0.01640 -0.0034  0.00225  0.00084 -0.005620  \n",
       "1993-05-31 -0.0166  0.01640 -0.0034  0.00225  0.00084 -0.005620  \n",
       "...            ...      ...     ...      ...      ...       ...  \n",
       "2015-05-31  0.0000  0.01239 -0.0034 -0.00286 -0.04040 -0.008213  \n",
       "2015-06-30  0.0000 -0.09222 -0.0034  0.00000 -0.01470 -0.005976  \n",
       "2015-07-31  0.0000  0.09483 -0.0034  0.00000 -0.00010 -0.023580  \n",
       "2015-08-31  0.0000 -0.05150 -0.0017  0.00000  0.03520  0.000000  \n",
       "2015-09-30  0.0000 -0.05150  0.0844  0.00000  0.03520  0.000000  \n",
       "\n",
       "[273 rows x 4501 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward and Backward fill imputation technique for missing values\n",
    "# Mean imputation technique has not been used to avoid bias and skewed results\n",
    "# The target variable is processed first, to continue to build the database hereafter based on this support variable \n",
    "returns_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'ROR')\n",
    "returns_df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d19763-bfd2-4bf7-8ee9-48e5d367cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukta\\AppData\\Local\\Temp\\ipykernel_33908\\1578932948.py:111: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dateaddedtotass_df = dateaddedtotass_df.ffill().bfill()\n",
      "C:\\Users\\yukta\\AppData\\Local\\Temp\\ipykernel_33908\\1578932948.py:121: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datedropped_df = datedropped_df.ffill().bfill()\n"
     ]
    }
   ],
   "source": [
    "# All the remaining columns have been processed to fill null values, either with forward backward fill or mean imputation\n",
    "''' Additionally each variable is split into different arrays indexed by the fund id, which later will be used to combine the \n",
    "fund characteristic data by fund id'''\n",
    "\n",
    "T_Bill_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'T_Bill')\n",
    "T_Bill_df = T_Bill_df.ffill().bfill()\n",
    "\n",
    "AUM_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'AUM_Filled')\n",
    "AUM_df = AUM_df.ffill().bfill()\n",
    "\n",
    "downside_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'downside')\n",
    "downside_df = downside_df.ffill().bfill()\n",
    "\n",
    "upside_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'upside')\n",
    "upside_df = upside_df.ffill().bfill()\n",
    "\n",
    "minimuminvestment_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'minimuminvestment')\n",
    "minimuminvestment_df = minimuminvestment_df.ffill().bfill()\n",
    "\n",
    "managementfee_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'managementfee')\n",
    "managementfee_df = managementfee_df.ffill().bfill()\n",
    "\n",
    "incentivefee_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'incentivefee')\n",
    "incentivefee_df = incentivefee_df.ffill().bfill()\n",
    "\n",
    "highwatermark_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'highwatermark')\n",
    "highwatermark_df = highwatermark_df.ffill().bfill()\n",
    "\n",
    "leveraged_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'leveraged')\n",
    "leveraged_df = leveraged_df.ffill().bfill()\n",
    "\n",
    "maxleverage_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'maxleverage')\n",
    "maxleverage_df = maxleverage_df.ffill().bfill()\n",
    "\n",
    "avgleverage_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'avgleverage')\n",
    "avgleverage_df = avgleverage_df.ffill().bfill()\n",
    "\n",
    "personalcapital_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'personalcapital')\n",
    "personalcapital_df = personalcapital_df.ffill().bfill()\n",
    "\n",
    "personalcapitalamount_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'personalcapitalamount')\n",
    "personalcapitalamount_df = personalcapitalamount_df.ffill().bfill()\n",
    "\n",
    "openended_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'openended')\n",
    "openended_df = openended_df.ffill().bfill()\n",
    "\n",
    "opentopublic_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'opentopublic')\n",
    "opentopublic_df = opentopublic_df.ffill().bfill()\n",
    "\n",
    "frequency_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'frequency')\n",
    "frequency_df = frequency_df.ffill().bfill()\n",
    "\n",
    "redemptionnoticeperiod_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'redemptionnoticeperiod')\n",
    "redemptionnoticeperiod_df = redemptionnoticeperiod_df.ffill().bfill()\n",
    "\n",
    "lockupperiod_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'lockupperiod')\n",
    "lockupperiod_df = lockupperiod_df.ffill().bfill()\n",
    "\n",
    "style_indicator_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'style_indicator')\n",
    "style_indicator_df = style_indicator_df.ffill().bfill()\n",
    "\n",
    "primarycategory_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'primarycategory')\n",
    "primarycategory_df = primarycategory_df.ffill().bfill()\n",
    "\n",
    "ConvArb_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'ConvArb')\n",
    "ConvArb_df = ConvArb_df.fillna(0)\n",
    "\n",
    "DedShort_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'DedShort')\n",
    "DedShort_df = DedShort_df.fillna(0)\n",
    "\n",
    "EmergingM_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'EmergingM')\n",
    "EmergingM_df = EmergingM_df.fillna(0)\n",
    "\n",
    "EquityMarketN_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'EquityMarketN')\n",
    "EquityMarketN_df = EquityMarketN_df.fillna(0)\n",
    "\n",
    "EventD_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'EventD')\n",
    "EventD_df = EventD_df.fillna(0)\n",
    "\n",
    "FixedIncArb_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'FixedIncArb')\n",
    "FixedIncArb_df = FixedIncArb_df.fillna(0)\n",
    "\n",
    "beta_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'beta')\n",
    "beta_df = beta_df.fillna(0)\n",
    "\n",
    "GlobalM_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'GlobalM')\n",
    "GlobalM_df = GlobalM_df.fillna(0)\n",
    "\n",
    "LongShortEq_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'LongShortEq')\n",
    "LongShortEq_df = LongShortEq_df.fillna(0)\n",
    "\n",
    "ManagedFut_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'ManagedFut')\n",
    "ManagedFut_df = ManagedFut_df.fillna(0)\n",
    "\n",
    "Other_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'Other')\n",
    "Other_df = Other_df.fillna(0)\n",
    "\n",
    "backfill_dummy_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'backfill_dummy')\n",
    "backfill_dummy_df = backfill_dummy_df.ffill().bfill()\n",
    "\n",
    "alive_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'alive')\n",
    "alive_df = alive_df.ffill().bfill()\n",
    "\n",
    "dropreason_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'dropreason')\n",
    "dropreason_df = dropreason_df.fillna('-')\n",
    "\n",
    "inceptiondate_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'inceptiondate')\n",
    "inceptiondate_df = inceptiondate_df.fillna(0)\n",
    "\n",
    "dateaddedtotass_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'dateaddedtotass')\n",
    "dateaddedtotass_df = dateaddedtotass_df.ffill().bfill()\n",
    "\n",
    "performancestartdate_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'performancestartdate')\n",
    "performancestartdate_df = performancestartdate_df.ffill().bfill()\n",
    "\n",
    "performanceenddate_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'performanceenddate')\n",
    "performanceenddate_df = performanceenddate_df.ffill().bfill()\n",
    "\n",
    "datedropped_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'datedropped')\n",
    "datedropped_df = datedropped_df.ffill().bfill()\n",
    "\n",
    "style_return_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'style_return')\n",
    "style_return_df = style_return_df.ffill().bfill()\n",
    "\n",
    "st_adj_rateofreturn_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'st_adj_rateofreturn')\n",
    "st_adj_rateofreturn_df = st_adj_rateofreturn_df.ffill().bfill()\n",
    "\n",
    "DeltaMgr_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'DeltaMgr')\n",
    "DeltaMgr_df = DeltaMgr_df.ffill().bfill()\n",
    "\n",
    "DeltaOwn_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'DeltaOwn')\n",
    "DeltaOwn_df = DeltaOwn_df.ffill().bfill()\n",
    "\n",
    "TotalDelta_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'TotalDelta')\n",
    "TotalDelta_df = TotalDelta_df.ffill().bfill()\n",
    "\n",
    "TotalDelta_Winso_01_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'TotalDelta_Winso_01')\n",
    "TotalDelta_Winso_01_df = TotalDelta_Winso_01_df.ffill().bfill()\n",
    "\n",
    "Netcashflow_df = TASS.pivot(index = 'refdate', columns = 'ref', values = 'Net cash flow')\n",
    "Netcashflow_df = Netcashflow_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebf75fb-916c-4e99-b88b-73c07c91dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The fund characteristics indexed by fund id above are combined into different dataframes, each specific to a fund, and stored in a dictionary \n",
    "with fund id as the key. Furthermore, the characteristic variables are converted to numeric format using one hot encoding before adding to the database'''\n",
    "fund_dfs1 = {}\n",
    "\n",
    "for i in returns_df.columns:\n",
    "    dfx = pd.DataFrame()\n",
    "    dfx[f'{i}_return'] = returns_df[i]\n",
    "    dfx[f'{i}_sma5'] = returns_df[i].rolling(window=5).mean()\n",
    "    dfx[f'{i}_sma9'] = returns_df[i].rolling(window=9).mean()\n",
    "    dfx[f'{i}_sma17'] = returns_df[i].rolling(window=17).mean()\n",
    "    dfx[f'{i}_downside'] = downside_df[i]\n",
    "    dfx[f'{i}_upside'] = upside_df[i]\n",
    "    dfx[f'{i}_mgmtfee'] = managementfee_df[i]\n",
    "    dfx[f'{i}_lvrgdfee'] = leveraged_df[i]\n",
    "    dfx[f'{i}_inctfee'] = incentivefee_df[i]\n",
    "    dfx[f'{i}_hwm'] = highwatermark_df[i]\n",
    "    dfx[f'{i}_lockuperiod'] = lockupperiod_df[i]\n",
    "    dfx[f'{i}_AUM'] = AUM_df[i]\n",
    "    dfx[f'{i}_tbill'] = T_Bill_df[i]\n",
    "    dfx[f'{i}_mininv'] = minimuminvestment_df[i]\n",
    "    dfx[f'{i}_maxlev'] = maxleverage_df[i]\n",
    "    dfx[f'{i}_avglev'] = avgleverage_df[i]\n",
    "    dfx[f'{i}_pcapamt'] = personalcapitalamount_df[i]\n",
    "    dfx[f'{i}_openended'] = openended_df[i]\n",
    "    dfx[f'{i}_opentopublic'] = opentopublic_df[i]\n",
    "    dfx[f'{i}_frequency'] = frequency_df[i]\n",
    "    dfx[f'{i}_redmnotcprd'] = redemptionnoticeperiod_df[i]\n",
    "    dfx[f'{i}_stylind'] = style_indicator_df[i]\n",
    "    dfx[f'{i}_ConvArb'] = ConvArb_df[i]\n",
    "    dfx[f'{i}_DedShort'] = DedShort_df[i]\n",
    "    dfx[f'{i}_EmergingM'] = EmergingM_df[i]\n",
    "    dfx[f'{i}_EquityMarketN'] = EquityMarketN_df[i]\n",
    "    dfx[f'{i}_EventD'] = EventD_df[i]\n",
    "    dfx[f'{i}_FixedIncArb'] = FixedIncArb_df[i]\n",
    "    dfx[f'{i}_GlobalM'] = GlobalM_df[i]\n",
    "    dfx[f'{i}_LongShortEq'] = LongShortEq_df[i]\n",
    "    dfx[f'{i}_ManagedFut'] = ManagedFut_df[i]\n",
    "    dfx[f'{i}_Other'] = Other_df[i]\n",
    "    dfx[f'{i}_beta'] = beta_df[i]\n",
    "    dfx[f'{i}_backfill'] = backfill_dummy_df[i]\n",
    "    dfx[f'{i}_alive'] = alive_df[i]\n",
    "    \n",
    "    # One-hot encode only 'dropreason'\n",
    "    dropreason_col = f'{i}_dropreason'\n",
    "    dfx[dropreason_col] = dropreason_df[i]\n",
    "    dfx = pd.get_dummies(dfx, columns=[dropreason_col], prefix=dropreason_col, drop_first=True)\n",
    "    \n",
    "    # Convert date columns to numeric (timestamp)\n",
    "    date_cols = {\n",
    "        f'{i}_inceptiondate': inceptiondate_df[i],\n",
    "        f'{i}_dateaddedtotass': dateaddedtotass_df[i],\n",
    "        f'{i}_performancestartdate': performancestartdate_df[i],\n",
    "        f'{i}_performanceenddate': performanceenddate_df[i],\n",
    "        f'{i}_datedropped': datedropped_df[i]\n",
    "    }\n",
    "\n",
    "    for col_name, series in date_cols.items():\n",
    "        dfx[col_name] = pd.to_datetime(series, errors='coerce').astype('int64') // 10**9  # Unix timestamp in seconds\n",
    "\n",
    "   \n",
    "    dfx[f'{i}_NCF'] = Netcashflow_df[i]\n",
    "\n",
    "    dfx.index = pd.to_datetime(returns_df.index)\n",
    "\n",
    "    fund_dfs1[i] = dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc781cd-8d71-49a2-9b69-c7b674551181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukta\\miniconda3\\Lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:348: UserWarning: y residual is constant at iteration 7\n",
      "  warnings.warn(f\"y residual is constant at iteration {k}\")\n",
      "C:\\Users\\yukta\\miniconda3\\Lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:348: UserWarning: y residual is constant at iteration 7\n",
      "  warnings.warn(f\"y residual is constant at iteration {k}\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pls_results = {}\n",
    "\n",
    "for fund_id, df in fund_dfs1.items():\n",
    "    df = df.dropna()\n",
    "    if df.shape[0] < 24:  # Ensure enough data\n",
    "        continue\n",
    "\n",
    "    # Target variable\n",
    "    y = df[f'{fund_id}_return']\n",
    "    X = df.drop(columns=[f'{fund_id}_return'])\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit PLS model (e.g., 5 components)\n",
    "    pls = PLSRegression(n_components=9)\n",
    "    pls.fit(X_scaled, y)\n",
    "\n",
    "    # Store weights (importance) of original features\n",
    "    feature_weights = pd.Series(np.abs(pls.coef_.flatten()), index=X.columns)\n",
    "    sorted_weights = feature_weights.sort_values(ascending=False)\n",
    "\n",
    "    pls_results[fund_id] = sorted_weights\n",
    "#Now pls_results[fund_id] contains the most predictive features based on PLS coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb0e0f8-47ee-4b47-84cf-1175f9b9da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build filtered dictionary with relevant features\n",
    "fund_dfs2 = {}\n",
    "\n",
    "for fund_id, df in fund_dfs1.items():\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    return_col = f\"{fund_id}_return\"\n",
    "    if return_col not in df.columns:\n",
    "        continue  # Skip if return column is missing\n",
    "    base_features = [\n",
    "    f\"{fund_id}_upside\", f\"{fund_id}_downside\", f\"{fund_id}_tbill\", f\"{fund_id}_sma5\", f\"{fund_id}_sma9\",\n",
    "    f\"{fund_id}_sma17\", f\"{fund_id}_NCF\", f\"{fund_id}_AUM\"\n",
    "]\n",
    "    # Include return column in relevant features\n",
    "    features_to_keep = [col for col in base_features if col in df.columns]\n",
    "    features_to_keep.append(return_col)\n",
    "\n",
    "    # Filter and drop NaNs\n",
    "    dfx = df[features_to_keep].dropna()\n",
    "    if not dfx.empty:\n",
    "        fund_dfs2[fund_id] = dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15091228-d612-4972-99a2-50d2d6b206ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joblib library employed for parallel processing of large data, about 4500 dataframes, efficiently\n",
    "# Ridge regression and Random forest\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def forecast_for_fund(fund_id, df):\n",
    "    models = {\n",
    "        'ridge': Ridge(alpha=1.0),\n",
    "        'random_forest': RandomForestRegressor(n_estimators=100, max_depth=5),\n",
    "    }\n",
    "\n",
    "    df = df.sort_index()\n",
    "    # Locate the return column dynamically\n",
    "    return_col = [col for col in df.columns if col.endswith('_return')]\n",
    "    if not return_col:\n",
    "        return []\n",
    "\n",
    "    df = df.rename(columns={return_col[0]: 'return'}).dropna()\n",
    "\n",
    "\n",
    "    df = df.rename(columns={return_col[0]: 'return'}).dropna()\n",
    "    if df.shape[0] < 24: #Filtering to exclude funds have less than 24 datapoints\n",
    "        return []\n",
    "\n",
    "    features = df.drop(columns=['return'])\n",
    "    target = df['return']\n",
    "    results = []\n",
    "\n",
    "    for test_date in pd.date_range(start='2005-01-01', end='2015-12-31', freq='Q'):\n",
    "        train_data = df.loc['1993-01-01':test_date - pd.DateOffset(months=3)]\n",
    "        if train_data.shape[0] < 36 or test_date not in df.index:\n",
    "            continue\n",
    "\n",
    "        X_train = train_data.drop(columns=['return'])\n",
    "        y_train = train_data['return']\n",
    "        X_test = features.loc[[test_date]]\n",
    "        y_test = target.loc[[test_date]]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            pred = pipe.predict(X_test)\n",
    "\n",
    "            results.append({\n",
    "                'fund_id': fund_id,\n",
    "                'model': model_name,\n",
    "                'test_date': test_date,\n",
    "                'actual': y_test.values[0],\n",
    "                'predicted': pred[0],\n",
    "                'mse': mean_squared_error(y_test, [pred[0]])\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "def rolling_forecast_2005_2015_parallel(fund_data):\n",
    "    all_results = Parallel(n_jobs=2)(  # Use all available CPU cores\n",
    "        delayed(forecast_for_fund)(fund_id, df) for fund_id, df in tqdm(fund_data.items())\n",
    "    )\n",
    "    # Flatten the list of lists\n",
    "    return pd.DataFrame([res for sublist in all_results for res in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5f0cf8-c15b-4b41-bc7f-0329b34b61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3336/3336 [1:09:42<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fund_id          model  test_date  actual  predicted           mse\n",
      "0           26          ridge 2005-03-31 -0.0067  -0.001650  2.550103e-05\n",
      "1           26  random_forest 2005-03-31 -0.0067  -0.009050  5.523163e-06\n",
      "2           26          ridge 2005-06-30  0.0149   0.013645  1.574827e-06\n",
      "3           26  random_forest 2005-06-30  0.0149   0.014287  3.757177e-07\n",
      "4           26          ridge 2005-09-30  0.0192   0.018806  1.549501e-07\n",
      "...        ...            ...        ...     ...        ...           ...\n",
      "60227   105138  random_forest 2005-06-30  0.0001   0.000180  6.379753e-09\n",
      "60228   105138          ridge 2005-09-30 -0.0102  -0.005608  2.108578e-05\n",
      "60229   105138  random_forest 2005-09-30 -0.0102  -0.008898  1.695676e-06\n",
      "60230   105138          ridge 2005-12-31 -0.0075  -0.003650  1.482277e-05\n",
      "60231   105138  random_forest 2005-12-31 -0.0075  -0.008561  1.126465e-06\n",
      "\n",
      "[5394 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Forecast on the test period of 2005 to 2015\n",
    "forecast_df = rolling_forecast_2005_2015_parallel(fund_dfs2)\n",
    "# Filter only the rows from the year 2005 for evaluation\n",
    "preds_2005 = forecast_df[forecast_df['test_date'].dt.year == 2005]\n",
    "\n",
    "# Display top 5 predictions for 2005\n",
    "print(preds_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d2e0fc-c518-4760-a306-dd20210493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso and Elastic Net\n",
    "def forecast_for_fund(fund_id, df):\n",
    "    models = {\n",
    "        'lasso': Lasso(alpha=0.01),\n",
    "        'elasticnet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    }\n",
    "\n",
    "    df = df.sort_index()\n",
    "    return_col = [col for col in df.columns if 'return' in col.lower()]\n",
    "    if not return_col:\n",
    "        return []\n",
    "\n",
    "    df = df.rename(columns={return_col[0]: 'return'}).dropna()\n",
    "    if df.shape[0] < 24:\n",
    "        return []\n",
    "\n",
    "    features = df.drop(columns=['return'])\n",
    "    target = df['return']\n",
    "    results = []\n",
    "\n",
    "    for test_date in pd.date_range(start='2005-01-01', end='2015-12-31', freq='ME'):\n",
    "        train_data = df.loc['1993-01-01':test_date - pd.DateOffset(months=1)]\n",
    "        if train_data.shape[0] < 36 or test_date not in df.index:\n",
    "            continue\n",
    "\n",
    "        X_train = train_data.drop(columns=['return'])\n",
    "        y_train = train_data['return']\n",
    "        X_test = features.loc[[test_date]]\n",
    "        y_test = target.loc[[test_date]]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            pred = pipe.predict(X_test)\n",
    "\n",
    "            results.append({\n",
    "                'fund_id': fund_id,\n",
    "                'model': model_name,\n",
    "                'test_date': test_date,\n",
    "                'actual': y_test.values[0],\n",
    "                'predicted': pred[0],\n",
    "                'mse': mean_squared_error(y_test, [pred[0]])\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def rolling_forecast_2005_2015_parallel(fund_data):\n",
    "    all_results = Parallel(n_jobs=2)(  # Use all available CPU cores\n",
    "        delayed(forecast_for_fund)(fund_id, df) for fund_id, df in tqdm(fund_data.items())\n",
    "    )\n",
    "    # Flatten the list of lists\n",
    "    return pd.DataFrame([res for sublist in all_results for res in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77bcbd0-8cc5-484a-8680-ca345b515c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3336/3336 [10:06<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        fund_id       model  test_date  actual  predicted           mse\n",
      "0            26       lasso 2005-01-31 -0.0132   0.006691  3.956443e-04\n",
      "1            26  elasticnet 2005-01-31 -0.0132   0.009275  5.051371e-04\n",
      "2            26       lasso 2005-02-28 -0.0035   0.007106  1.124824e-04\n",
      "3            26  elasticnet 2005-02-28 -0.0035   0.009275  1.632071e-04\n",
      "4            26       lasso 2005-03-31 -0.0067   0.006644  1.780504e-04\n",
      "...         ...         ...        ...     ...        ...           ...\n",
      "187443   105138  elasticnet 2005-10-31  0.0010   0.001535  2.862945e-07\n",
      "187444   105138       lasso 2005-11-30  0.0153   0.002304  1.688989e-04\n",
      "187445   105138  elasticnet 2005-11-30  0.0153   0.001535  1.894734e-04\n",
      "187446   105138       lasso 2005-12-31 -0.0075   0.001286  7.718784e-05\n",
      "187447   105138  elasticnet 2005-12-31 -0.0075   0.001703  8.468659e-05\n",
      "\n",
      "[16610 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "forecast_df = rolling_forecast_2005_2015_parallel(fund_dfs2)\n",
    "# Filter only the rows from the year 2005\n",
    "preds1_2005 = forecast_df[forecast_df['test_date'].dt.year == 2005]\n",
    "\n",
    "# Display top 5 predictions for 2005\n",
    "print(preds1_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba17d589-3b79-41d9-bed4-01b8b8bcbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree, Gradient Boosting, Deep Neural Network\n",
    "def forecast_for_fund(fund_id, df):\n",
    "    models = {\n",
    "        'decision_tree': DecisionTreeRegressor(max_depth=5),\n",
    "        'gradient_boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1),\n",
    "        'dnn': MLPRegressor(hidden_layer_sizes=(50, 20), max_iter=500)\n",
    "    }\n",
    "\n",
    "    df = df.sort_index()\n",
    "    return_col = [col for col in df.columns if 'return' in col.lower()]\n",
    "    if not return_col:\n",
    "        return []\n",
    "\n",
    "    df = df.rename(columns={return_col[0]: 'return'}).dropna()\n",
    "    if df.shape[0] < 24:\n",
    "        return []\n",
    "\n",
    "    features = df.drop(columns=['return'])\n",
    "    target = df['return']\n",
    "    results = []\n",
    "\n",
    "    for test_date in pd.date_range(start='2005-01-01', end='2015-12-31', freq='ME'):\n",
    "        train_data = df.loc['1993-01-01':test_date - pd.DateOffset(months=1)]\n",
    "        \n",
    "        if train_data.shape[0] < 36 or test_date not in df.index:\n",
    "            continue\n",
    "\n",
    "        X_train = train_data.drop(columns=['return'])\n",
    "        y_train = train_data['return']\n",
    "        X_test = features.loc[[test_date]]\n",
    "        y_test = target.loc[[test_date]]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            pred = pipe.predict(X_test)\n",
    "\n",
    "            results.append({\n",
    "                'fund_id': fund_id,\n",
    "                'model': model_name,\n",
    "                'test_date': test_date,\n",
    "                'actual': y_test.values[0],\n",
    "                'predicted': pred[0],\n",
    "                'mse': mean_squared_error(y_test, [pred[0]])\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def rolling_forecast_2005_2015_parallel(fund_data):\n",
    "    all_results = Parallel(n_jobs=2)(  # Use all available CPU cores\n",
    "        delayed(forecast_for_fund)(fund_id, df) for fund_id, df in tqdm(fund_data.items())\n",
    "    )\n",
    "    # Flatten the list of lists\n",
    "    return pd.DataFrame([res for sublist in all_results for res in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ede7426f-f32b-48fd-897f-068fa3dac6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3336/3336 [1:28:23<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        fund_id              model  test_date  actual  predicted           mse\n",
      "0            26      decision_tree 2005-01-31 -0.0132  -0.008330  2.371691e-05\n",
      "1            26  gradient_boosting 2005-01-31 -0.0132  -0.013018  3.309711e-08\n",
      "2            26                dnn 2005-01-31 -0.0132   0.003908  2.926864e-04\n",
      "3            26      decision_tree 2005-02-28 -0.0035   0.001575  2.575867e-05\n",
      "4            26  gradient_boosting 2005-02-28 -0.0035  -0.000454  9.280692e-06\n",
      "...         ...                ...        ...     ...        ...           ...\n",
      "281167   105138  gradient_boosting 2005-11-30  0.0153   0.013304  3.985605e-06\n",
      "281168   105138                dnn 2005-11-30  0.0153   0.101342  7.403194e-03\n",
      "281169   105138      decision_tree 2005-12-31 -0.0075  -0.009200  2.890000e-06\n",
      "281170   105138  gradient_boosting 2005-12-31 -0.0075  -0.008142  4.121403e-07\n",
      "281171   105138                dnn 2005-12-31 -0.0075   0.050662  3.382776e-03\n",
      "\n",
      "[24915 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "forecast_df = rolling_forecast_2005_2015_parallel(fund_dfs2)\n",
    "# Filter only the rows from the year 2005\n",
    "preds2_2005 = forecast_df[forecast_df['test_date'].dt.year == 2005]\n",
    "\n",
    "# Display top 5 predictions for 2005\n",
    "print(preds2_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603f7a3b-1b17-497c-94ee-c61ee55ae286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0124560909358782\n",
      "0.13656905551534448\n",
      "6.781944984780712\n",
      "1.7989703799823331\n",
      "1.027948829829629\n",
      "0.409248966383943\n",
      "1.3881530103178536e+16\n"
     ]
    }
   ],
   "source": [
    "#Compare prediction accracy of all models with measn squared error\n",
    "print(preds_2005[preds_2005['model'] == 'ridge']['mse'].sum())\n",
    "print(preds_2005[preds_2005['model'] =='random_forest']['mse'].sum())\n",
    "print(preds1_2005[preds1_2005['model'] =='elasticnet']['mse'].sum())\n",
    "print(preds1_2005[preds1_2005['model'] =='lasso']['mse'].sum())\n",
    "print(preds2_2005[preds2_2005['model'] =='decision_tree']['mse'].sum())\n",
    "print(preds2_2005[preds2_2005['model'] =='gradient_boosting']['mse'].sum())\n",
    "print(preds2_2005[preds2_2005['model'] =='dnn']['mse'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b39bfc-63bc-493d-af82-9d3ca33a4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalizing on the Gradient Boosting algorithm as our final model due to processing efficiency and prediction accuracy\n",
    "final_preds = forecast_df[forecast_df['model'] == 'gradient_boosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac47897-bc4e-40e9-8686-02950155cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1866,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique funds after filtering is 1866\n",
    "list = pd.Series(forecast_df['fund_id'].unique())\n",
    "list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41d1ab28-978d-44b4-b9de-82dc2422dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds.head()\n",
    "pred_ret_funds = final_preds.pivot(index = 'test_date', columns = 'fund_id', values = 'predicted')\n",
    "pred_ret_funds = pred_ret_funds.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d80be05-f098-4f00-b7d3-305e0342984f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fund_id\n",
       "26        0\n",
       "31        0\n",
       "35        0\n",
       "61        0\n",
       "78        0\n",
       "         ..\n",
       "105116    0\n",
       "105136    0\n",
       "105138    0\n",
       "105709    0\n",
       "105785    0\n",
       "Length: 1866, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a copy of baseline predictions\n",
    "baseline_q = pred_ret_funds.copy()     \n",
    "\n",
    "pred_ret_funds.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe133489-87e4-47fa-ab66-7bccbdbfdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbill = TASS[['refdate', 'T_Bill']].copy()  # Safe copy\n",
    "tbill['refdate'] = pd.to_datetime(tbill['refdate'])  # No warning now\n",
    "tbill.set_index('refdate', inplace=True)\n",
    "tbill_q = tbill.loc[pred_ret_funds.index, 'T_Bill']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "237a52fc-a4cd-4fa6-9d29-7cf58b5106a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_logs/PPO_7\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 225  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 198       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 20        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9563425 |\n",
      "|    clip_fraction        | 0.841     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65e+03 |\n",
      "|    explained_variance   | -3.25     |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.0818   |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -0.138    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 189       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 32        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.177165  |\n",
      "|    clip_fraction        | 0.84      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65e+03 |\n",
      "|    explained_variance   | 0.548     |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.144    |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.14     |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.0907    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 185       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 44        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8269573 |\n",
      "|    clip_fraction        | 0.843     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65e+03 |\n",
      "|    explained_variance   | 0.786     |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.142    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.0676    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 184       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 55        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6457975 |\n",
      "|    clip_fraction        | 0.845     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65e+03 |\n",
      "|    explained_variance   | 0.888     |\n",
      "|    learning_rate        | 5e-05     |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.144    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.0498    |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# ======== Custom Portfolio Environment ========\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, predicted_returns, baseline_returns, tbill_rate,cov_matrix, window_size=1):\n",
    "        super().__init__()\n",
    "        self.predicted_returns = predicted_returns.sort_index()\n",
    "        self.baseline_returns = baseline_returns.sort_index()\n",
    "        self.tbill_rate = tbill_rate.sort_index()\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.window_size = window_size\n",
    "        self.funds = self.predicted_returns.columns.tolist()\n",
    "        self.n_funds = len(self.funds)\n",
    "        self.periods = self.predicted_returns.index\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Observation space: predicted returns for each fund\n",
    "        max_abs_val = np.max(np.abs(self.predicted_returns.values))\n",
    "        self.observation_space = spaces.Box(low=-max_abs_val, high=max_abs_val, shape=(self.n_funds,), dtype=np.float32)\n",
    "\n",
    "        # Action space: portfolio weights between [0, 1], normalized later\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_funds,), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def reset(self, seed=0, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.prev_weights = np.ones(self.n_funds) / self.n_funds\n",
    "        obs = self.predicted_returns.iloc[self.current_step].values.astype(np.float32)\n",
    "        obs = np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        action = np.array(action)\n",
    "        abs_sum = np.sum(np.abs(action))\n",
    "        if abs_sum < 1e-8:\n",
    "            weights = np.zeros_like(action)\n",
    "        else:\n",
    "            weights = action / abs_sum\n",
    "\n",
    "    \n",
    "        # Compute return\n",
    "        ret = np.dot(weights, self.predicted_returns.iloc[self.current_step].values)\n",
    "        base_ret = np.dot(self.prev_weights, self.baseline_returns.iloc[self.current_step].values)\n",
    "    \n",
    "        excess_ret = ret - self.tbill_rate.iloc[self.current_step]\n",
    "        base_excess = base_ret - self.tbill_rate.iloc[self.current_step]\n",
    "    \n",
    "        # Risk penalty term (portfolio variance)\n",
    "        # cov_matrix is prepared and aligned with fund order\n",
    "        risk_penalty_factor = 0.01  # Tune hyperparameter\n",
    "    \n",
    "        portfolio_variance = weights.T @ self.cov_matrix @ weights\n",
    "        risk_penalty = risk_penalty_factor * portfolio_variance\n",
    "    \n",
    "        # Transaction cost\n",
    "        transaction_cost_rate = 0.0005\n",
    "        turnover = np.sum(np.abs(weights - self.prev_weights))\n",
    "        transaction_cost = transaction_cost_rate * turnover\n",
    "    \n",
    "        # Reward: excess return minus baseline excess return, minus transaction cost and risk penalty\n",
    "        reward = excess_ret - base_excess - transaction_cost - risk_penalty\n",
    "    \n",
    "        self.prev_weights = weights\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.predicted_returns)\n",
    "    \n",
    "        if not done:\n",
    "            obs = self.predicted_returns.iloc[self.current_step].values.astype(np.float32)\n",
    "            obs = np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "        else:\n",
    "            obs = np.zeros(self.n_funds, dtype=np.float32)\n",
    "    \n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "\n",
    "# ======== Load and Prepare Data ========\n",
    "# baseline_returns, predicted_returns, tbill_rate are pandas DataFrames/Series aligned on index\n",
    "# Compute covariance matrix for baseline returns\n",
    "cov_matrix = baseline_q.cov().loc[pred_ret_funds.columns, pred_ret_funds.columns].values\n",
    "\n",
    "# Create environment instance with covariance matrix\n",
    "env = PortfolioEnv(pred_ret_funds, baseline_q, tbill_q, cov_matrix)\n",
    "\n",
    "# ======== Instantiate and Check Env ========\n",
    "raw_env = PortfolioEnv(pred_ret_funds, baseline_q, tbill_q, cov_matrix)\n",
    "check_env(raw_env)\n",
    "\n",
    "# ======== Create Vectorized and Normalized Env ========\n",
    "vec_env = DummyVecEnv([lambda: PortfolioEnv(pred_ret_funds, baseline_q, tbill_q, cov_matrix)])\n",
    "\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "# ======== Train PPO Agent ========\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    learning_rate=5e-5,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_logs/\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# ======== Save Trained Model and Normalization Stats ========\n",
    "model.save(\"ppo_portfolio_model\")\n",
    "vec_env.save(\"vec_normalize.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e981b96-59ad-4fb2-90a1-755e2d974be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              26        31        35        61        78        86      \\\n",
      "Date                                                                     \n",
      "2005-03-31  0.001068  0.000560  0.000324  0.001068  0.001068  0.000569   \n",
      "2005-04-30  0.000923  0.000798  0.000571  0.000622  0.000618  0.001070   \n",
      "2005-05-31  0.000980  0.000540  0.000529  0.000276  0.000943  0.000678   \n",
      "2005-06-30  0.000000  0.000406  0.000858  0.000499  0.000300  0.000410   \n",
      "2005-07-31  0.000000  0.000023  0.000880  0.000140  0.000437  0.000487   \n",
      "\n",
      "              87        151       167       184     ...    104216    104252  \\\n",
      "Date                                                ...                       \n",
      "2005-03-31  0.001068  0.001068  0.000990  0.001068  ...  0.000864  0.000819   \n",
      "2005-04-30  0.000922  0.000771  0.000237  0.001070  ...  0.000000  0.000339   \n",
      "2005-05-31  0.001057  0.000418  0.000743  0.000256  ...  0.000181  0.000280   \n",
      "2005-06-30  0.000225  0.000837  0.001071  0.000790  ...  0.000000  0.000889   \n",
      "2005-07-31  0.001054  0.000963  0.000403  0.000063  ...  0.000174  0.000630   \n",
      "\n",
      "              104341    104573    104674    105116    105136    105138  \\\n",
      "Date                                                                     \n",
      "2005-03-31  0.001068  0.001068  0.000000  0.001068  0.000616  0.000409   \n",
      "2005-04-30  0.001070  0.000000  0.001070  0.000554  0.000042  0.000000   \n",
      "2005-05-31  0.000951  0.000892  0.001057  0.001057  0.001057  0.000496   \n",
      "2005-06-30  0.000424  0.000000  0.000543  0.000499  0.000000  0.000593   \n",
      "2005-07-31  0.000807  0.000051  0.001054  0.000966  0.000883  0.000927   \n",
      "\n",
      "              105709    105785  \n",
      "Date                            \n",
      "2005-03-31  0.000323  0.000023  \n",
      "2005-04-30  0.000992  0.000000  \n",
      "2005-05-31  0.000219  0.000129  \n",
      "2005-06-30  0.000000  0.000642  \n",
      "2005-07-31  0.000641  0.000292  \n",
      "\n",
      "[5 rows x 1866 columns]\n"
     ]
    }
   ],
   "source": [
    "# Final weight allocation to different funds with reinforcement learning with reward as Sharpe Ratio, model created above\n",
    "# Allocation output as an excel file\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Recreate env and load normalization stats\n",
    "test_env = DummyVecEnv([lambda: PortfolioEnv(pred_ret_funds, baseline_q, tbill_q, cov_matrix)])\n",
    "test_env = VecNormalize.load(\"vec_normalize.pkl\", test_env)\n",
    "test_env.training = False\n",
    "test_env.norm_reward = False\n",
    "\n",
    "# Load model\n",
    "model = PPO.load(\"ppo_portfolio_model\")\n",
    "\n",
    "# Evaluate\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "portfolio_history = []\n",
    "dates = env.periods[env.window_size:] \n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Convert action to final weights as used in the environment\n",
    "    weights_raw = (action + 1) / 2\n",
    "    weights = weights_raw / np.sum(weights_raw) if np.sum(weights_raw) > 1e-8 else np.ones(env.n_funds) / env.n_funds\n",
    "    \n",
    "    # Store the current date and weights\n",
    "    step_index = env.current_step - 1  # Because we incremented after step\n",
    "    if step_index < len(dates):\n",
    "        date = dates[step_index]\n",
    "        portfolio_history.append((date, weights))\n",
    "# Convert to DataFrame\n",
    "df_weights = pd.DataFrame(\n",
    "    [w for d, w in portfolio_history],\n",
    "    index=[d for d, w in portfolio_history],\n",
    "    columns=env.funds\n",
    ")\n",
    "\n",
    "# Save or inspect\n",
    "df_weights.index.name = \"Date\"\n",
    "df_weights.to_csv(\"final_portfolios_2005_2015.csv\")\n",
    "print(df_weights.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
